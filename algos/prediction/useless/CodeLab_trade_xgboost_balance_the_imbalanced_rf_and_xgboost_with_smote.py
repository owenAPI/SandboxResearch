# -*- coding: utf-8 -*-
"""Trade_xgboost_balance-the-imbalanced-rf-and-xgboost-with-smote.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/104c__CRrWFkX02VD662g0cdZpl_yGnrb

# Fraud analysis: 
### Random Forest, XGBoost, OneClassSVM, Multivariate GMM and SMOTE, all in one cage against an imbalanced dataset.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import warnings
warnings.filterwarnings('ignore')

"""## 1. Supervised learning tests

I will now test a series of different machine learning Models (no Neural Networks!) to see which one performs better, with some optimization here and there.

### 1.1 Data import, quick view and functions
"""

df = pd.read_csv("d_price/FAV_SCALA_stock_history_E_MONTH_3.csv",index_col=False, sep='\t')

df.head()

"""I wonder if it should be treated as a data series rather than a table... """

numeric_feature_names = ['buy_sell_point','ticker',
                         "Date","Open","High","Low","Close","Volume","per_Close","per_Volume","per_preMarket","olap_BBAND_UPPER","olap_BBAND_MIDDLE",
                         "olap_BBAND_LOWER","olap_HT_TRENDLINE","olap_MIDPOINT","olap_MIDPRICE","olap_SAR","olap_SAREXT","mtum_ADX","mtum_ADXR","mtum_APO",
                         "mtum_AROON_down","mtum_AROON_up","mtum_AROONOSC","mtum_BOP","mtum_CCI","mtum_CMO","mtum_DX","mtum_MACD","mtum_MACD_signal","mtum_MACD_list",
                         "mtum_MACD_ext","mtum_MACD_ext_signal","mtum_MACD_ext_list","mtum_MACD_fix","mtum_MACD_fix_signal","mtum_MACD_fix_list","mtum_MFI",
                         "mtum_MINUS_DI","mtum_MINUS_DM","mtum_MOM","mtum_PLUS_DI","mtum_PLUS_DM","mtum_PPO","mtum_ROC","mtum_ROCP","mtum_ROCR","mtum_ROCR100",
                         "mtum_RSI","mtum_STOCH_k","mtum_STOCH_d","mtum_STOCH_kd","mtum_STOCH_Fa_k","mtum_STOCH_Fa_d","mtum_STOCH_Fa_kd","mtum_STOCH_RSI_k",
                         "mtum_STOCH_RSI_d","mtum_STOCH_RSI_kd","mtum_ULTOSC","mtum_WILLIAMS_R","volu_Chaikin_AD","volu_Chaikin_ADOSC","volu_OBV",
                         "vola_ATR","vola_NATR","vola_TRANGE","cycl_DCPERIOD","cycl_DCPHASE","cycl_PHASOR_inph","cycl_PHASOR_quad","cycl_SINE_sine","cycl_SINE_lead", # "cycl_HT_TRENDMODE",
                         "sti_BETA","sti_CORREL","sti_LINEARREG","sti_LINEARREG_ANGLE","sti_LINEARREG_INTERCEPT",
                         "sti_LINEARREG_SLOPE","sti_STDDEV","sti_TSF","sti_VAR","ma_DEMA_5","ma_EMA_5","ma_KAMA_5","ma_SMA_5","ma_T3_5","ma_TEMA_5","ma_TRIMA_5",
                         "ma_WMA_5","ma_DEMA_10","ma_EMA_10","ma_KAMA_10","ma_SMA_10","ma_T3_10","ma_TEMA_10","ma_TRIMA_10","ma_WMA_10","ma_DEMA_20","ma_EMA_20",
                         "ma_KAMA_20","ma_SMA_20","ma_TEMA_20","ma_TRIMA_20","ma_WMA_20","ma_EMA_50","ma_KAMA_50","ma_SMA_50","ma_TRIMA_50",
                         # TOO MACH null data in this columns ,"ma_DEMA_50","ma_T3_20","mtum_TRIX"
                        #  "ma_WMA_50","ma_EMA_100","ma_KAMA_100","ma_SMA_100","ma_TRIMA_100","ma_WMA_100",
                         "trad_s3","trad_s2","trad_s1","trad_pp","trad_r1","trad_r2",
                         "trad_r3","clas_s3","clas_s2","clas_s1","clas_pp","clas_r1","clas_r2","clas_r3","fibo_s3","fibo_s2","fibo_s1","fibo_pp","fibo_r1","fibo_r2",
                         "fibo_r3","wood_s3","wood_s2","wood_s1","wood_pp","wood_r1","wood_r2","wood_r3","demark_s1","demark_pp","demark_r1","cama_s3","cama_s2",
                         "cama_s1","cama_pp","cama_r1","cama_r2","cama_r3","ti_acc_dist","ti_chaikin_10_3","ti_choppiness_14","ti_coppock_14_11_10",
                         "ti_donchian_lower_20","ti_donchian_center_20","ti_donchian_upper_20","ti_ease_of_movement_14","ti_force_index_13","ti_hma_20",
                         "ti_kelt_20_lower","ti_kelt_20_upper","ti_mass_index_9_25","ti_supertrend_20","ti_vortex_pos_5","ti_vortex_neg_5","ti_vortex_pos_14","ti_vortex_neg_14"]

candle_columns = ["cdl_2CROWS","cdl_3BLACKCROWS","cdl_3INSIDE","cdl_3LINESTRIKE","cdl_3OUTSIDE","cdl_3STARSINSOUTH","cdl_3WHITESOLDIERS",
                         "cdl_ABANDONEDBABY","cdl_ADVANCEBLOCK","cdl_BELTHOLD","cdl_BREAKAWAY","cdl_CLOSINGMARUBOZU","cdl_CONCEALBABYSWALL","cdl_COUNTERATTACK",
                         "cdl_DARKCLOUDCOVER","cdl_DOJI","cdl_DOJISTAR","cdl_DRAGONFLYDOJI","cdl_ENGULFING","cdl_EVENINGDOJISTAR","cdl_EVENINGSTAR",
                         "cdl_GAPSIDESIDEWHITE","cdl_GRAVESTONEDOJI","cdl_HAMMER","cdl_HANGINGMAN","cdl_HARAMI","cdl_HARAMICROSS","cdl_HIGHWAVE","cdl_HIKKAKE",
                         "cdl_HIKKAKEMOD","cdl_HOMINGPIGEON","cdl_IDENTICAL3CROWS","cdl_INNECK","cdl_INVERTEDHAMMER","cdl_KICKING","cdl_KICKINGBYLENGTH",
                         "cdl_LADDERBOTTOM","cdl_LONGLEGGEDDOJI","cdl_LONGLINE","cdl_MARUBOZU","cdl_MATCHINGLOW","cdl_MATHOLD","cdl_MORNINGDOJISTAR","cdl_MORNINGSTAR",
                         "cdl_ONNECK","cdl_PIERCING","cdl_RICKSHAWMAN","cdl_RISEFALL3METHODS","cdl_SEPARATINGLINES","cdl_SHOOTINGSTAR","cdl_SHORTLINE",
                         "cdl_SPINNINGTOP","cdl_STALLEDPATTERN","cdl_STICKSANDWICH","cdl_TAKURI","cdl_TASUKIGAP","cdl_THRUSTING","cdl_TRISTAR","cdl_UNIQUE3RIVER",
                         "cdl_UPSIDEGAP2CROWS","cdl_XSIDEGAP3METHODS"]

columns_valids = numeric_feature_names + candle_columns

df[columns_valids].describe()

#https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/
def plot_confusion_matrix(cf_matrix, path = None, title_str = 'Seaborn Confusion Matrix with labels\n\n' ):
    plt.figure(figsize=(6,6))

    labels = ['True Neg','False Pos','False Neg','True Pos']
    labels = np.asarray(labels).reshape(2,2)

    cm_sum = np.sum(cf_matrix, axis=1, keepdims=True)
    cm_perc = cf_matrix / cm_sum.astype(float) * 100
    annot = np.empty_like(cf_matrix).astype(str)
    nrows, ncols = cf_matrix.shape
    for i in range(nrows):
        for j in range(ncols):
            c = cf_matrix[i, j]
            p = cm_perc[i, j]
            if i == j:
                s = cm_sum[i]
                annot[i, j] = '%.1f%%\n%d/%d' % (p, c, s)
            elif c == 0:
                annot[i, j] = ''
            else:
                annot[i, j] = '%.1f%%\n%d' % (p, c)
    cm = pd.DataFrame(cf_matrix, index=labels, columns=labels)
    cm.index.name = 'Actual'
    cm.columns.name = 'Predicted'

    #ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')
    # ax = sns.heatmap(cf_matrix / np.sum(cf_matrix), annot=True,
    #                  fmt='.2%', cmap='Blues')
    ax =  sns.heatmap(cm, annot=annot, fmt='', cmap='Blues')

    ax.set_title(title_str);
    ax.set_xlabel('\nPredicted Values')
    ax.set_ylabel('Actual Values ');

    ## Ticket labels - List must be in alphabetical order
    ax.xaxis.set_ticklabels(['False','True'])
    ax.yaxis.set_ticklabels(['False','True'])


    ## Display the visualization of the Confusion Matrix.
    # plt.show()
    if path is not None:
        print("plot_confusion_matrix  " + path)
        plt.savefig(path)

def __autopct_fun(abs_values):
    #Source: https://www.holadevs.com/pregunta/64169/adding-absolute-values-to-the-labels-of-each-portion-of-matplotlibpyplotpie
    gen = iter(abs_values)
    return lambda pct: f"{pct:.1f}% ({next(gen)})"
def plot_pie_countvalues(df, colum_count , stockid= "", opion = ""):
    df =  df.groupby(colum_count).count()
    y = np.array(df['Date'])

    plt.figure()
    plt.pie( y , labels=df.index, autopct=__autopct_fun(y),startangle=9, shadow=True)

    name = stockid + "_"+colum_count+"_"+opion
    plt.title("Count times values:\n"+ name)
    plt.savefig("pie_plot_"+ name + ".png")

"""Check for NaNs"""

cleaned_df = df.copy()

# You don't want the `Time` column.
cleaned_df['Date'] = pd.to_datetime(cleaned_df['Date']).map(pd.Timestamp.timestamp)
cleaned_df = cleaned_df[columns_valids]
cleaned_df = pd.get_dummies(cleaned_df, columns = [ 'ticker'])
df = cleaned_df.dropna()

"""WOW! Seriously, no NaNs? 

Ok, let's check for the classes distributions
"""

Y_TARGET = 'buy_sell_point'
Y_target_classes = df[Y_TARGET].unique().tolist()
print(f"Label classes: {Y_target_classes}")
df[Y_TARGET] = df[Y_TARGET].map(Y_target_classes.index)

neg, pos = np.bincount(df[Y_TARGET])
total = neg + pos
print('Examples:\n    Total: {}\n    Positive: {} ({:.2f}% of total)\n'.format(
    total, pos, 100 * pos / total))

plot_pie_countvalues(df,Y_TARGET )

print('Fraud \n',df.Date[df[Y_TARGET]==1].describe(),'\n',
      '\n Non-Fraud \n',df.Date[df[Y_TARGET]==0].describe())

"""Imbalanced dataset. Might be worth to work on upsampling/downsampling of the data, but I will try without it for the moment and hope I get good results. Now let's check which variable is more correlated with the fraudulent activities. """

df.isnull().sum()



"""Data preparation and general functions for plots"""

from sklearn.metrics import confusion_matrix
def plot_cm(classifier, predictions,path = None, p=0.5):
    cm = confusion_matrix(y_test, predictions > p)
    plot_confusion_matrix(cm, path, title_str = 'Confusion matrix @{:.2f}'.format(p) )

    tn, fp, fn, tp = cm.ravel()

    recall = tp / (tp + fn)
    precision = tp / (tp + fp)
    F1 = 2*recall*precision/(recall+precision)

    print('Recall={0:0.3f}'.format(recall),'\nPrecision={0:0.3f}'.format(precision))
    print('F1={0:0.3f}'.format(F1))


    print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])
    print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])
    print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])
    print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])
    print('Total Fraudulent Transactions: ', np.sum(cm[1]))

from sklearn.metrics import average_precision_score, precision_recall_curve
def plot_aucprc(classifier, scores,path = None):
    plt.figure()
    precision, recall, _ = precision_recall_curve(y_test, scores, pos_label=0)
    average_precision = average_precision_score(y_test, scores)

    print('Average precision-recall score: {0:0.3f}'.format(
          average_precision))

    plt.plot(recall, precision, label='area = %0.3f' % average_precision, color="green")
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision Recall Curve')
    plt.legend(loc="best")
    if path is not None:
        print("plot Average precision-recall  " + path)
        plt.savefig(path)

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X = df.iloc[:,:-1]
y = df.iloc[:,-1]


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, shuffle=False)

print("Shapes  X: ", X.shape, "  Y: ", y.shape)

"""## 1.2. Test a Random Forest model"""

# Fitting Random Forest Classification to the Training set
from sklearn.ensemble import RandomForestClassifier

pre = RandomForestClassifier(n_jobs=-1,
                             max_features= 'sqrt', 
                             criterion = 'entropy')
pre.fit(X_train, y_train)

#Make predictions
y_pred = pre.predict(X_test)
try:
    scores = pre.decision_function(X_test)
except:
    scores = pre.predict_proba(X_test)[:,1]

#Make plots
plot_cm(pre, y_pred, path= "RandomForestClassifier_confusion_matrix.png")
plot_aucprc(pre, scores, path= "RandomForestClassifier_aucprc.png")

"""The F-1 score is not that bad! Let's try to fine tune some parameters and see if we can improve that.

*Note: since it takes too long for the Kaggle kernel, I executed it on my computer and here I am just showing the results of the GridSearchCV*

Exhaustive search over specified parameter values for an estimator.

Important members are fit, predict.

GridSearchCV es una clase disponible en scikit-learn que permite evaluar y seleccionar de forma sistemática los parámetros de un modelo. Indicándole un modelo y los parámetros a probar, puede evaluar el rendimiento del primero en función de los segundos mediante validación cruzada. En caso de que se desee evaluar modelos con parámetros aleatorios existe el método RandomizedSearchCV.
"""

# from sklearn.model_selection import GridSearchCV
# param_grid = { 
#    'n_estimators': [10, 500],
#    'max_features': ['auto', 'sqrt', 'log2'],
#    'min_samples_leaf' : [len(X)//10000, len(X)//28000, 
#                          len(X)//50000, len(X)//100000]
# }

# CV_rfc = GridSearchCV(estimator=pre, 
#                      param_grid=param_grid, 
#                      scoring = 'f1',
#                      cv=10, 
#                      n_jobs=10,
#                      verbose=2,
#                      pre_dispatch='2*n_jobs',
#                      refit=False)
# CV_rfc.fit(X_train, y_train)

# print("Exhaustive search over specified parameter values for an estimator.")
# print(CV_rfc.best_params_)
# Fitting 10 folds for each of 24 candidates, totalling 240 fits
# Exhaustive search over specified parameter values for an estimator.
# {'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 500}

#rfc = RandomForestClassifier(n_jobs=-1, random_state = 42,
#                             n_estimators=CV_rfc.best_params_['n_estimators'], 
#                             min_samples_leaf=CV_rfc.best_params_['min_samples_leaf'], 
#                             max_features= CV_rfc.best_params_['max_features'])

#RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
#            max_depth=None, max_features='auto', max_leaf_nodes=None,
#            min_impurity_split=1e-07, min_samples_leaf=2,
#            min_samples_split=2, min_weight_fraction_leaf=0.0,
#            n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,
#            verbose=0, warm_start=False)


rfc = RandomForestClassifier(n_jobs=-1,# random_state = 42,
                             n_estimators=500, 
                             max_features='sqrt',
                             min_samples_leaf=1,
                             criterion = 'entropy')

rfc.fit(X_train, y_train)

#Make predictions
y_pred = rfc.predict(X_test)
try:
    scores = rfc.decision_function(X_test)
except:
    scores = rfc.predict_proba(X_test)[:,1]

#Make plots
plot_cm(pre, y_pred, path= "RandomForestClassifier_GridSearch_confusion_matrix.png")
plot_aucprc(pre, scores, path= "RandomForestClassifier_GridSearch_aucprc.png")

"""Yass! Nice increase! Now let's see if I can get any better with the most popular boosting algorithm...

## 1.3. Test a XGBoost model
"""

# Fitting XGBoost to the Training set
from xgboost import XGBClassifier
xgb = XGBClassifier(random_state = 42, n_jobs = -1)
xgb.fit(X_train, y_train)

#Make predictions
y_pred = xgb.predict(X_test)
try:
    scores = xgb.decision_function(X_test)
except:
    scores = xgb.predict_proba(X_test)[:,1]
#Make plots
y_pred = xgb.predict(X_test)
plot_cm(pre, y_pred, path= "XGBClassifier_confusion_matrix.png")
plot_aucprc(pre, scores, path= "XGBClassifier_aucprc.png")

"""Ok, now we're talking. Any chances of getting better with optimization?"""

fraud_ratio=y_train.value_counts()[1]/y_train.value_counts()[0]
from sklearn.model_selection import GridSearchCV
param_grid = {'max_depth': [1,3,5], 
             'min_child_weight': [1,3,5], 
             'n_estimators': [100,200,500,1000], 
             'scale_pos_weight': [1, 0.1, 0.01, fraud_ratio]}
# {'max_depth': 1, 'min_child_weight': 1, 'n_estimators': 500, 'scale_pos_weight': 1}
# [05:20:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627:
# Parameters: { "criterion" } might not be used.
CV_GBM = GridSearchCV(estimator = xgb, 
                     param_grid = param_grid,
                     scoring = 'f1', 
                     cv = 10, 
                     n_jobs = -1,
                     refit = True) 
 
CV_GBM.fit(X_train, y_train, verbose=2)

CV_GBM.best_params_
print(CV_GBM.best_params_)

#optimized_GBM = XGBClassifier(n_jobs=-1, random_state = 42,
#                             n_estimators=CV_GBM.best_params_['n_estimators'], 
#                             max_depth=CV_GBM.best_params_['max_depth'],
#                             min_child_weight=CV_GBM.best_params_['min_child_weight'],
#                             criterion = 'entropy')
optimized_GBM = XGBClassifier(n_jobs=-1,
                             n_estimators=500,
                             max_depth=1,
                             min_child_weight=1,
                             criterion = 'entropy',
                             scale_pos_weight=1)
optimized_GBM.fit(X_train, y_train)

#Make predictions
y_pred = optimized_GBM.predict(X_test)
try:
    scores = optimized_GBM.decision_function(X_test)
except:
    scores = optimized_GBM.predict_proba(X_test)[:,1]
    
#Make plots
plot_cm(pre, y_pred, path= "XGBClassifier_GridSearch_confusion_matrix.png")
plot_aucprc(pre, scores, path= "XGBClassifier_GridSearch_aucprc.png")

"""## 1.4. OneClassSVM?

This should be, according to Scikit-learn tutorials, the best algorithm to infer anomalies in an imbalanced dataset. Let's give it a try.
"""

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Fitting SVM to the Training set
from sklearn.svm import OneClassSVM
classifier = OneClassSVM(kernel="rbf", random_state = 42)
classifier.fit(X_train, y_train)

#Make predictions
y_pred = classifier.predict(X_test)
y_pred = np.array([y==-1 for y in y_pred])

try:
    scores = classifier.decision_function(X_test)
except:
    scores = classifier.predict_proba(X_test)[:,1]

#Make plots
plot_cm(classifier, y_pred)
plot_aucprc(classifier, scores)

"""I don't really like these results, honestly...

## 1.5. Test a (Multivariate) GMM module

Per Ng's lessons, we should divide the dataset into a training set with ONLY normal transactions, and validation and test set containing all the fraudulent transations. I will try at first without crossvalidation.
"""

df3 = df2#.sample(frac = 0.1, random_state=42)
train = df3[df3.Class==0].sample(frac=0.75, random_state = 42)

X_train = train.iloc[:,:-1]
y_train = train.iloc[:,-1]

X_test = df3.loc[~df3.index.isin(X_train.index)].iloc[:,:-1]#.sample(frac=.50, random_state = 42)
y_test = df3.loc[~df3.index.isin(y_train.index)].iloc[:,-1]#.sample(frac=.50, random_state = 42)

#X_cval = df3.loc[~df3.index.isin(X_test.index)& ~df3.index.isin(X_train.index)].iloc[:,:-1]
#y_cval = df3.loc[~df3.index.isin(y_test.index)& ~df3.index.isin(X_train.index)].iloc[:,-1]

print('df3', df3.shape,'\n',
      'train',train.shape,'\n',
      'X_train',X_train.shape,'\n',
      'y_train',y_train.shape,'\n',
      'X_test',X_test.shape,'\n',
      'y_test',y_test.shape,'\n', 
      #'X_val',X_cval.shape,'\n',
      #'y_val',y_cval.shape,'\n'
     )

df3.shape[0] == train.shape[0] + X_test.shape[0]

def covariance_matrix(X):
    X = X.values
    m, n = X.shape 
    tmp_mat = np.zeros((n, n))
    mu = X.mean(axis=0)
    for i in range(m):
        tmp_mat += np.outer(X[i] - mu, X[i] - mu)
    return tmp_mat / m

cov_mat = covariance_matrix(X_train)

cov_mat_inv = np.linalg.pinv(cov_mat)
cov_mat_det = np.linalg.det(cov_mat)
def multi_gauss(x):
    n = len(cov_mat)
    return (np.exp(-0.5 * np.dot(x, np.dot(cov_mat_inv, x.T))) 
            / (2. * np.pi)**(n/2.) 
            / np.sqrt(cov_mat_det))

eps = min([multi_gauss(x) for x in X_train.values])
predictions = np.array([multi_gauss(x) <= eps for x in X_test.values])
y_test = np.array(y_test, dtype=bool)

cm = confusion_matrix(y_test, predictions)

plt.clf()
plt.imshow(cm, interpolation='nearest', cmap='RdBu')
classNames = ['Normal','Fraud']
plt.ylabel('True label')
plt.xlabel('Predicted label')
tick_marks = np.arange(len(classNames))
plt.xticks(tick_marks, classNames, rotation=45)
plt.yticks(tick_marks, classNames)
s = [['TN','FP'], ['FN', 'TP']]

for i in range(2):
    for j in range(2):
        plt.text(j,i, str(s[i][j])+" = "+str(cm[i][j]), 
                 horizontalalignment='center', color='White')

plt.show()

tn, fp, fn, tp = cm.ravel()

recall = tp / (tp + fn)
precision = tp / (tp + fp)
F1 = 2*recall*precision/(recall+precision)

print("recall=",recall,"\nprecision=",precision)
print("F1=",F1)

"""Adapting NG's code from MatLab"""

from scipy.stats import multivariate_normal
from sklearn.metrics import f1_score

def feature_normalize(dataset):
    mu = np.mean(dataset,axis=0)
    sigma = np.std(dataset,axis=0)
    return (dataset - mu)/sigma

def estimateGaussian(dataset):
    mu = np.mean(dataset, axis=0)
    sigma = np.cov(dataset.T)
    return mu, sigma
    
def multivariateGaussian(dataset,mu,sigma):
    p = multivariate_normal(mean=mu, cov=sigma, allow_singular=True)
    return p.pdf(dataset)

def selectThresholdByCV(probs,gt):
    best_epsilon = 0
    best_f1 = 0
    f = 0
    stepsize = (max(probs) - min(probs)) / 1000;
    epsilons = np.arange(min(probs),max(probs),stepsize)
    for epsilon in np.nditer(epsilons):
        predictions = (probs < epsilon)
        f = f1_score(gt, predictions, average = "binary")
        if f > best_f1:
            best_f1 = f
            best_epsilon = epsilon
    return best_f1, best_epsilon

#fit the model
mu, sigma = estimateGaussian(X_train)
p = multivariateGaussian(X_train,mu,sigma)

p_cv = multivariateGaussian(X_test,mu,sigma)
fscore, ep = selectThresholdByCV(p_cv,y_test)
outliers = np.asarray(np.where(p < ep))

predictions = np.array([p_cv <= ep]).transpose()
y_test = np.array(y_test, dtype=bool)

cm = confusion_matrix(y_test, predictions)

plt.clf()
plt.imshow(cm, interpolation='nearest', cmap='RdBu')
classNames = ['Normal','Fraud']
plt.ylabel('True label')
plt.xlabel('Predicted label')
tick_marks = np.arange(len(classNames))
plt.xticks(tick_marks, classNames, rotation=45)
plt.yticks(tick_marks, classNames)
s = [['TN','FP'], ['FN', 'TP']]

for i in range(2):
    for j in range(2):
        plt.text(j,i, str(s[i][j])+" = "+str(cm[i][j]), 
                 horizontalalignment='center', color='White')

plt.show()

tn, fp, fn, tp = cm.ravel()

recall = tp / (tp + fn)
precision = tp / (tp + fp)
F1 = 2*recall*precision/(recall+precision)

print("recall=",recall,"\nprecision=",precision)
print("F1=",F1)

"""## 2.  Working on imbalanced dataset: upsampling of the underrepresented class

## 2.1 BalancedBaggingClassifier

The package imbalanced-learn (not yet part of the official scikitlearn) contains an imbalanced classifier which should be able, using a bagging method, to increase our prediction capabilities without resampling the dataset.

First, let's reset our original dataset, without the unwanted features.
"""

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X = df2.iloc[:,:-1]
y = df2.iloc[:,-1]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)

from sklearn.ensemble import BaggingClassifier
from imblearn.ensemble import BalancedBaggingClassifier

from imblearn.metrics import classification_report_imbalanced

bagging = BaggingClassifier(random_state=0)
balanced_bagging = BalancedBaggingClassifier(random_state=0)

bagging.fit(X_train, y_train)
balanced_bagging.fit(X_train, y_train)

#Make predictions
print('Classification of original dataset with Bagging (scikit-learn)')
y_pred = bagging.predict(X_test)
try:
    scores = bagging.decision_function(X_test)
except:
    scores = bagging.predict_proba(X_test)[:,1]

#Make plots
plot_cm(bagging, y_pred)
plot_aucprc(bagging, scores)

#Make predictions
print('Classification of original dataset with BalancedBagging (imbalanced-learn)')
y_pred = balanced_bagging.predict(X_test)
try:
    scores = balanced_bagging.decision_function(X_test)
except:
    scores = balanced_bagging.predict_proba(X_test)[:,1]

#Make plots
plot_cm(balanced_bagging, y_pred)
plot_aucprc(balanced_bagging, scores)

"""So, the imbalanced-learn packages without resampling of the data allows us to have higher true positive numbers, but lowers the true negative, meaning that it's just mistakenly saying there are more frauds than in reality. Not good.  
Let's try again, with SMOTE, which will produce synthetical samples of the under-represented class
"""

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_sample(X, y)

X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.25, random_state = 42)

#fit the best Models so far
xgb.fit(X_train, y_train)
rfc.fit(X_train, y_train)

#Make predictions
print('Classification of SMOTE-resampled dataset with XGboost')
y_pred = xgb.predict(X_test)
try:
    scores = xgb.decision_function(X_test)
except:
    scores = xgb.predict_proba(X_test)[:,1]
#Make plots
y_pred = xgb.predict(X_test)
plot_cm(xgb, y_pred)
plot_aucprc(xgb, scores)

#Make predictions
print('Classification of SMOTE-resampled dataset with optimized RF')
y_pred = rfc.predict(X_test)
try:
    scores = rfc.decision_function(X_test)
except:
    scores = rfc.predict_proba(X_test)[:,1]

#Make plots
plot_cm(rfc, y_pred)
plot_aucprc(rfc, scores)

"""WOW! So, if we now use this new RF classifier (which parameters were optimized on the resampled dataset) on the ORIGINAL dataset, we'll get our perfect fraud analysis prediction."""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)

#Make predictions
y_pred = rfc.predict(X_test)
try:
    scores = rfc.decision_function(X_test)
except:
    scores = rfc.predict_proba(X_test)[:,1]

#Make plots
plot_cm(rfc, y_pred)
plot_aucprc(rfc, scores)



